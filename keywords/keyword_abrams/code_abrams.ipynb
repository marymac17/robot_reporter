{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data cleaning \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import initial libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data \n",
    "\n",
    "df = pd.read_csv('data/abrams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at basic info about data\n",
    "\n",
    "df.info()\n",
    "# this data set consists of 6824 Tweets\n",
    "# Twarc search for keyword \"Twitter\", the morning after Democratic victories in Georgia Senate races \n",
    "# Stacey Abrams was widely credited with bringing about Georgia transition from red to blue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of languages in data set\n",
    "\n",
    "count_lang = df['lang'].unique()\n",
    "print(len(count_lang), count_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets are in 23 different languages\n",
    "\n",
    "# I'll be working only with Tweets in English\n",
    "# drop tweets in all other languages\n",
    "# now working with 6659 Tweets \n",
    "\n",
    "df = df[df.lang == 'en']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns \n",
    "\n",
    "df = df.drop(['tweet_url', 'created_at', 'media', 'urls','in_reply_to_screen_name',\n",
    "       'in_reply_to_status_id', 'in_reply_to_user_id', 'retweet_or_quote_id',\n",
    "       'retweet_or_quote_screen_name', 'retweet_or_quote_user_id', 'source',\n",
    "       'user_created_at', 'user_name', 'user_verified', 'user_friends_count', 'user_listed_count',\n",
    "       'user_statuses_count', 'user_default_profile_image', 'user_description',\n",
    "       'user_favourites_count', 'user_followers_count', 'coordinates', 'lang', 'user_location', 'user_time_zone', 'user_urls', 'place'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check start time & date of data\n",
    "\n",
    "df.iloc[0]\n",
    "\n",
    "# first Tweet downloaded Jan 6, 2021 at 18:42:31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check end time & date of data \n",
    "\n",
    "df.iloc[-1]\n",
    "\n",
    "# last Tweet on Jan 6, 2021 at 19:02:01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Text processing for NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variable for \"text\" column \n",
    "\n",
    "text = df['text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize, remove stopwords, remove urls, lowercase, remove punctuation, remove numbers\n",
    "# import necessary libraries: ntlk etc.\n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "punc = list(set(string.punctuation))\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def remove_url(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r'', text)\n",
    "\n",
    "def process_text(text):\n",
    "    text = remove_url(text)\n",
    "    text = tokenizer(text)\n",
    "    text = [word.lower() for word in text]\n",
    "    text = [re.sub('[0-9]+', '', word) for word in text]\n",
    "    text = [word for word in text if word not in punc]\n",
    "    text = [word for word in text if word not in stop]\n",
    "    text = [each for each in text if len(each) > 1]\n",
    "    text = [word for word in text if ' ' not in word]\n",
    "     \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply text processing functions to text\n",
    "\n",
    "df['processed_text'] = df['text'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at some of processed text\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "df['processed_text'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part-of-speech tagging \n",
    "\n",
    "ready_for_pos = df['processed_text']\n",
    "\n",
    "def pos_tagging(text):\n",
    "    pos_tag = [pos_tag(word) for word in ready_for_pos]\n",
    "\n",
    "df['pos_tagged'] = df.processed_text.apply(lambda x: pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizing\n",
    "\n",
    "pos_tagged = df['pos_tagged']\n",
    "\n",
    "wordnet = WordNetLemmatizer() \n",
    "\n",
    "lemmatized = [[wordnet.lemmatize(word[0]) for word in words] for words in pos_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at lemmatized text\n",
    "\n",
    "df['lemmatized'] = lemmatized\n",
    "lemmatized[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before vectorizing, cast lists of words back into strings\n",
    "\n",
    "df['final_docs'] = df['lemmatized'].apply(lambda x: \" \".join(x))\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "final_docs = df['final_docs']\n",
    "final_docs[3000:3020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create document term matrix with TFIDF\n",
    "\n",
    "#import vectorizing tool (usee TFIDF)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# set max_features to 2000 (specifies the number of most frequently occurring words for which we want to create feature vectors)\n",
    "# set min_df to 5 (word must occur in at least 5 documents)\n",
    "# set max_df to 0.85 (word must not occur in more than 85 percent of the documents) \n",
    "\n",
    "tfidfconverter = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.85, ngram_range=(1, 2), stop_words='english')  \n",
    "doc_term_matrix_1 = tfidfconverter.fit_transform(df['final_docs'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: run NMF and LDA models, for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run NMF model \n",
    "\n",
    "#import NMF tool \n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf_model = NMF(n_components=6)\n",
    "nmf_Z = nmf_model.fit_transform(doc_term_matrix_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run LDA model\n",
    "\n",
    "#import LDA tool \n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components = 6, max_iter=10, learning_method='online', learning_decay=.9)\n",
    "lda_Z = lda_model.fit_transform(doc_term_matrix_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]])\n",
    " \n",
    "print(\"LDA Model:\")\n",
    "print_topics(lda_model, tfidfconverter )\n",
    "print(\"=\" * 20)\n",
    " \n",
    "print(\"NMF Model:\")\n",
    "print_topics(nmf_model, tfidfconverter )\n",
    "print(\"=\" * 20)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Run visualization and testing of LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood:  -167849.4323281318\n",
      "Perplexity:  920.0496458340432\n"
     ]
    }
   ],
   "source": [
    "# test LDA model\n",
    "\n",
    "# log likelihood (higher score is better)\n",
    "print(\"Log likelihood: \", lda_model.score(doc_term_matrix_1))\n",
    "\n",
    "# perplexity (lower score is better)\n",
    "print(\"Perplexity: \", lda_model.perplexity(doc_term_matrix_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el35111402784079817765761016317\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el35111402784079817765761016317_data = {\"mdsDat\": {\"x\": [1.4650187492370605, 54.85479736328125, 32.212928771972656, 5.111096382141113, 26.350170135498047, 38.10981750488281], \"y\": [-47.625064849853516, -36.4767951965332, -61.44457244873047, -14.110331535339355, -33.37470245361328, -7.222781181335449], \"topics\": [1, 2, 3, 4, 5, 6], \"cluster\": [1, 1, 1, 1, 1, 1], \"Freq\": [18.647874787321207, 17.844918261075733, 17.625090910658585, 16.537566573553985, 15.195712955639701, 14.148836511750781]}, \"tinfo\": {\"Term\": [\"tweet\", \"carrying\", \"abrams tweet\", \"abrams carrying\", \"carrying democracy\", \"state\", \"election stacey\", \"abrams state\", \"block\", \"spun\", \"spun block\", \"follow lead\", \"crazy\", \"invest\", \"trust\", \"state stacey\", \"invest follow\", \"listen trust\", \"trust invest\", \"em crazy\", \"em\", \"block em\", \"abrams spun\", \"state listen\", \"follow\", \"lead\", \"democracy\", \"stole election\", \"stole\", \"listen\", \"owe\", \"real\", \"american hero\", \"worked\", \"tell\", \"saved democracy\", \"abrams american\", \"jensenackles\", \"jarpad\", \"supporter\", \"thing know\", \"house\", \"hero\", \"sure\", \"politician\", \"maybe\", \"date\", \"team\", \"huge\", \"covid\", \"entire\", \"world thank\", \"biden victory\", \"let know\", \"stacey saved\", \"know date\", \"jarpad stacey\", \"imaginary world\", \"jensenackles jarpad\", \"democracy real\", \"world saved\", \"world\", \"saved\", \"time\", \"know\", \"say\", \"staceyabrams\", \"thank\", \"thank stacey\", \"let\", \"vote\", \"right\", \"want\", \"black\", \"people\", \"joe\", \"black woman\", \"stacey\", \"stacey abrams\", \"year\", \"woman\", \"thing\", \"think\", \"democrat\", \"american\", \"georgia\", \"like\", \"white\", \"queen\", \"kemp\", \"response\", \"vision\", \"harris\", \"politics\", \"actually\", \"turned\", \"lesson\", \"brian kemp\", \"brian\", \"mobilization\", \"governor kemp\", \"needed\", \"focus\", \"state blue\", \"followed\", \"georgia blue\", \"abrams said\", \"declined\", \"declined run\", \"reform\", \"work georgia\", \"thank goodness\", \"goodness\", \"clearly\", \"deserves\", \"kemp stole\", \"stolen\", \"activist\", \"blue\", \"stacy abrams\", \"stacy\", \"need\", \"leader\", \"said\", \"moment\", \"run\", \"lot\", \"saving\", \"got\", \"georgia\", \"governor\", \"thank\", \"state\", \"work\", \"staceyabrams\", \"stacey abrams\", \"voter\", \"stacey\", \"voting\", \"election\", \"office\", \"win\", \"senate\", \"abrams tweet\", \"tweet\", \"election stacey\", \"spun\", \"block\", \"spun block\", \"em crazy\", \"em\", \"abrams spun\", \"block em\", \"crazy\", \"talk stacey\", \"sound\", \"sound like\", \"dad\", \"make sound\", \"asking talk\", \"like dad\", \"way make\", \"asking\", \"determined\", \"fucking abrams\", \"pessimistic\", \"mother\", \"stacey fucking\", \"optimistic\", \"mother abrams\", \"stacey mother\", \"pessimistic determined\", \"optimistic pessimistic\", \"stole election\", \"determined stacey\", \"stole\", \"talk\", \"abrams way\", \"election\", \"stacey\", \"stacey abrams\", \"like\", \"make\", \"way\", \"carrying\", \"abrams carrying\", \"carrying democracy\", \"abrams state\", \"follow lead\", \"invest\", \"trust\", \"invest follow\", \"state stacey\", \"listen trust\", \"trust invest\", \"state listen\", \"super\", \"senate seat\", \"believing\", \"abrams super\", \"super human\", \"believing stacey\", \"instead believing\", \"believe happen\", \"happen work\", \"human believe\", \"georgia senate\", \"spent decade\", \"infrastructure\", \"political landscape\", \"seat person\", \"shifting\", \"person responsible\", \"decade building\", \"follow\", \"lead\", \"listen\", \"democracy\", \"state\", \"stacey abrams\", \"stacey\", \"human\", \"georgia\", \"goddess\", \"abrams goddess\", \"important\", \"warnock jon\", \"far\", \"abrams raphael\", \"quite\", \"tea\", \"uk\", \"election georgia\", \"cat\", \"egg\", \"cat happiness\", \"happiness\", \"boiled\", \"tea cat\", \"boiled egg\", \"frankly\", \"absolutely quite\", \"soft boiled\", \"englishman uk\", \"cup\", \"happiness far\", \"georgia absolutely\", \"really day\", \"far important\", \"ossoff really\", \"englishman\", \"important stacey\", \"quite frankly\", \"really\", \"raphael\", \"raphael warnock\", \"warnock\", \"ossoff\", \"absolutely\", \"white\", \"day\", \"stacey\", \"stacey abrams\", \"jon ossoff\", \"jon\", \"consequential\", \"argument\", \"woman history\", \"abrams consequential\", \"good argument\", \"argument stacey\", \"consequential woman\", \"secretary\", \"secretary state\", \"signature\", \"pundit\", \"republican party\", \"motherfucking\", \"abrams cheated\", \"governor year\", \"year torched\", \"flipped motherfucking\", \"royalty\", \"party flipped\", \"torched republican\", \"senate american\", \"american royalty\", \"torched\", \"cheated governor\", \"motherfucking senate\", \"meaningful\", \"georgia republican\", \"match\", \"phone\", \"air\", \"history\", \"good\", \"republican\", \"party\", \"cheated\", \"stacey abrams\", \"stacey\", \"woman\", \"governor\", \"senate\", \"state\", \"going\"], \"Freq\": [121.0, 108.0, 112.0, 107.0, 107.0, 194.0, 83.0, 79.0, 82.0, 82.0, 82.0, 78.0, 82.0, 78.0, 78.0, 78.0, 78.0, 78.0, 78.0, 81.0, 81.0, 81.0, 81.0, 78.0, 80.0, 83.0, 121.0, 85.0, 88.0, 89.0, 29.049940755514733, 19.456678278799703, 17.60820966115267, 17.173813753233098, 16.676167868827115, 16.224321190734504, 14.358246429192558, 14.234713494221712, 14.23464554684611, 13.785967701427461, 12.838126463886649, 14.443094727814325, 32.856805370807805, 13.425682096250176, 15.84386844758398, 12.18194438994316, 12.092604291301825, 12.12701499505125, 12.01174074649864, 11.776590112968364, 11.818970700506673, 11.680510845680889, 11.682011116959789, 11.439497218623071, 11.439578081773178, 11.439624277991935, 11.439710509171379, 11.439897790507876, 11.43960692763016, 11.439729457818244, 11.440241106034476, 29.0421229149451, 36.42040513313604, 29.86604033011963, 36.169729429723766, 21.87014223577183, 51.95925701323134, 57.60715360501413, 22.38415606772307, 26.689169019649604, 24.093707895447398, 21.961481642382722, 30.477503210772717, 40.97370194615699, 31.011677882132304, 16.884260482461244, 26.25017818195298, 68.51592940709557, 66.4814774916327, 24.324061237484166, 33.53405350303776, 21.369627813561106, 18.331617728638797, 22.478263797343434, 23.711901772997226, 29.271070061958582, 22.65972145390676, 19.408050711143755, 36.881077983527256, 33.02104882004755, 19.571406510309043, 17.639881194102983, 15.717679695076601, 15.23648651214317, 14.48238574577541, 14.055581827324989, 13.664898980256917, 13.590851261780733, 13.590885553473216, 13.298835465415044, 13.189871397230641, 13.151332749497552, 13.15435633093348, 12.823561910987685, 12.619518881453923, 12.414152584175747, 12.881354501721898, 12.063748021848347, 12.063716713614044, 12.00640995984927, 11.96117023853525, 11.916889746262239, 11.916567768583885, 11.742747530275718, 11.782374660263999, 11.526258135250142, 11.609916080053335, 12.148958133093728, 30.46402269816933, 73.65387353588305, 74.43559424820953, 36.430660304043485, 29.122572320955907, 27.74457866069375, 16.657128746976195, 26.973129246775784, 19.851282559112953, 23.715402923066293, 22.124476209482413, 57.11076149462265, 27.759992589617696, 33.98520805533809, 42.947787550884705, 29.92381906342738, 27.81880667308683, 48.38490415035961, 23.860018728334804, 48.33972412411568, 15.416905982033656, 25.774274946444297, 15.955243407821111, 15.988466536100077, 17.310413368094633, 111.88110701057523, 120.8191845295069, 82.53760016507373, 81.9676492649114, 82.07278968180935, 81.96775286522613, 80.48795208107484, 80.48795286649067, 80.48782525259209, 80.48814404314788, 81.31314152672755, 32.1525059932961, 31.15166503204682, 30.11384118159746, 30.253946921785076, 29.944139988059014, 29.94348617127764, 29.943428179447466, 29.943606819667547, 30.941405476311225, 24.98322143760954, 24.644911335320913, 24.279020049768004, 24.825872286492697, 24.181321444536586, 24.14346933739931, 23.930205982492474, 23.93050191338766, 23.970964511195977, 23.97104297088446, 77.16561334467087, 23.971416392385606, 77.48671990726756, 31.926650719965284, 29.215887255571317, 60.71493300490553, 100.26713048195656, 89.68862381955121, 39.25847534176886, 31.45700457134131, 30.58913064482096, 108.0397390114952, 107.03632388357201, 107.03596821280013, 79.15700855341942, 77.91714264938183, 77.75875219764691, 77.64584583081634, 77.4510461326159, 77.61162737132159, 77.45124337075933, 77.45105906754547, 77.25632726058981, 31.178846330285026, 30.457992875796716, 30.174125378083154, 30.107317393298505, 29.993995064057323, 29.99394339807456, 29.99400169488488, 29.994038176079826, 29.993887053671717, 29.99410118990365, 28.377973535044298, 28.037445657271945, 27.947364493570543, 27.64150999202307, 27.641793247389234, 27.64156590201855, 27.641561056509396, 27.641259935343957, 78.33535593028992, 79.45204206547679, 73.7149637259671, 92.98244611315418, 119.61555490342339, 125.92184793554593, 121.81319595128163, 30.803959311249393, 33.30530337132198, 45.356213140412436, 44.914202893241054, 35.43592319382042, 32.915567773346936, 33.29846826016969, 31.93695682396442, 31.97746812841637, 31.705303870672537, 32.1740475968443, 31.39463656254298, 31.34725203784527, 31.22827596417688, 31.06964945103868, 31.06950929861733, 30.981036472207123, 30.98044884621698, 30.980602419389147, 30.92159503917211, 30.70185468949716, 30.701929629734, 30.702503250826975, 30.70214147926878, 30.702060528135153, 30.702336192809323, 30.701866846065663, 30.701969795984933, 30.70250526937184, 30.70197780284813, 30.70185655278839, 30.701921424581975, 45.71262076348737, 48.57471500822953, 48.574466659951725, 50.4321184702233, 43.50553517313177, 34.50714630345876, 41.09779784484121, 37.056286415400336, 40.476658180385165, 38.09992937945385, 31.085942927770983, 31.08583886803365, 30.097958863281107, 30.08622373555451, 28.92225226952792, 28.713817717236655, 28.714234853647383, 28.714359999150886, 28.71418783956395, 25.690324898268276, 25.419398373629935, 25.28263214974238, 23.582199870931667, 23.250419969258125, 20.944493075921788, 19.801357039071412, 19.328254485811176, 19.041003377557153, 19.040550192447817, 19.041160194100005, 19.04110250330599, 19.040916919683763, 19.040989621907368, 19.041271951177794, 19.04112379157642, 19.041268203924087, 19.040777712459857, 17.32661968334771, 17.560331455582745, 17.10477268912985, 17.21166473471228, 17.035127498768055, 32.398222781274555, 38.15806806897632, 35.45980414742167, 25.782056854444455, 25.27513814967862, 40.5141618908057, 38.660871673844134, 27.906480452685017, 24.02739159487957, 23.52573313502112, 26.803888131794267, 19.129908888379827], \"Total\": [121.0, 108.0, 112.0, 107.0, 107.0, 194.0, 83.0, 79.0, 82.0, 82.0, 82.0, 78.0, 82.0, 78.0, 78.0, 78.0, 78.0, 78.0, 78.0, 81.0, 81.0, 81.0, 81.0, 78.0, 80.0, 83.0, 121.0, 85.0, 88.0, 89.0, 29.86338328145093, 20.265511961998524, 18.404742490535664, 17.979391114665187, 17.478133599433132, 17.020773196945093, 15.154528170862406, 15.030282235976731, 15.0306914533227, 14.60273653683455, 13.642150025564925, 15.371421392745763, 34.97839364992078, 14.295175804346947, 16.87919447583111, 12.979449318480205, 12.890802828583741, 12.93292608266815, 12.815960778059825, 12.576312265964791, 12.621867656546868, 12.477155539673433, 12.47931841275324, 12.235058575634413, 12.235550651149236, 12.23560634515918, 12.235704275834943, 12.236080535993798, 12.23579702640903, 12.235990860021316, 12.2365696344675, 34.15584038907184, 44.50857317200885, 37.438213587212495, 47.51379181072244, 27.221311334419763, 80.49335233129172, 93.13589860232388, 28.37247621126786, 39.537140262328215, 34.367571244406534, 31.000122398633597, 52.004276476107044, 84.80884043970656, 60.92657112490684, 22.885614400870093, 53.73643133236902, 418.07350981867876, 409.09094466734905, 49.52968571994195, 101.92797971359609, 39.068852479554465, 27.936887510755486, 48.97936742452184, 64.14917572957268, 180.14447163473906, 83.22426649271436, 63.33163857326276, 37.714823297205534, 33.81860372261646, 20.364112867910045, 18.430182748825608, 16.509689362169198, 16.03790102339928, 15.278070973570461, 14.846733344167317, 14.459149451744617, 14.385884563906377, 14.386838917376812, 14.088357729948696, 13.979118997356517, 13.943778238339421, 13.950098260451185, 13.613409984068738, 13.409579631787823, 13.209909335727941, 13.709929301749938, 12.853606446049366, 12.853708562664956, 12.797078901584097, 12.751831713823384, 12.706109588445376, 12.707010500793235, 12.537198751639858, 12.586297972503354, 12.315374321834875, 12.405201677364204, 12.989163529510957, 33.92438616508165, 98.17956411886749, 99.96979155742923, 46.858135854467086, 38.22776528612175, 36.9750274220498, 19.94400411908955, 38.83128536918108, 26.520646774898918, 34.543820718845794, 34.27115537135461, 180.14447163473906, 58.26177815399393, 93.13589860232388, 194.51624475428093, 87.79999818316031, 80.49335233129172, 409.09094466734905, 64.0713484494065, 418.07350981867876, 22.062504940133447, 132.76064404965368, 31.617681169368495, 33.39858014953247, 73.76418111436965, 112.71827957647345, 121.9135709497425, 83.33372334110477, 82.76216485019674, 82.86891322957376, 82.76286393208618, 81.28219709370914, 81.28244153818756, 81.28255670278837, 81.28300471449916, 82.26036375490406, 32.93949664899364, 31.939559919946152, 30.901745940768873, 31.04830393857291, 30.731236867607382, 30.7309602385097, 30.73112510497431, 30.731660332846406, 31.762767667880762, 25.78011011928022, 25.43688509240723, 25.071358235384032, 25.637271869020758, 24.97250538057053, 24.934826064958425, 24.716915268222508, 24.71805665021411, 24.762137007752624, 24.76233845595558, 85.61834259479231, 24.762871423552504, 88.80612951131684, 38.148266358491334, 33.65932248231894, 132.76064404965368, 418.07350981867876, 409.09094466734905, 83.22426649271436, 51.14609348290513, 49.331187020285284, 108.83016238053573, 107.82582836307205, 107.82674423727605, 79.95290382660032, 78.71047520058136, 78.55145902688041, 78.43907464396375, 78.24339157833339, 78.40564496952686, 78.24370858345497, 78.24424856925897, 78.04980617597245, 31.971333609646475, 31.24923183815107, 30.963978615776828, 30.897802313923478, 30.78304182388312, 30.783332287993293, 30.78346729449628, 30.783642315029724, 30.783653616050188, 30.784046543648, 29.16942636909524, 28.826515541280784, 28.737212269620244, 28.430418308336684, 28.4308910958491, 28.43097834059264, 28.431051824051284, 28.430780404087038, 80.61118099406431, 83.05316678225486, 89.4062768303682, 121.09654063200587, 194.51624475428093, 409.09094466734905, 418.07350981867876, 35.10780654358165, 180.14447163473906, 46.12761456746613, 45.68506753109667, 36.25000440019743, 33.687437222303906, 34.0837516515244, 32.7087667928556, 32.75091747250461, 32.47628551344962, 32.959167688742625, 32.167865620455785, 32.11942494383622, 32.00063526989298, 31.840817459258584, 31.840905576529973, 31.752097255715473, 31.751909607277483, 31.752301369846265, 31.693139061803986, 31.47230359170584, 31.472819210698308, 31.473408447051206, 31.47319352211131, 31.47342159542759, 31.47372900946029, 31.47328898673365, 31.473399141914854, 31.47399764815674, 31.47345935214827, 31.473401228135934, 31.47351857215897, 52.54334154120479, 59.418992880960914, 59.41898949025685, 70.33010746902262, 61.94637929476456, 40.985317739498015, 63.33163857326276, 54.81173272745905, 418.07350981867876, 409.09094466734905, 42.79421910079428, 42.7947583373972, 30.874341176074804, 30.862973214521134, 29.698075506699777, 29.48980803869715, 29.490364069991717, 29.490648634254377, 29.49069659299992, 26.47400765868703, 26.197693749579525, 26.066683957995018, 24.35838593726925, 24.045546595478747, 21.723909351234436, 20.580010756452637, 20.107262267567968, 19.81947223543734, 19.819021247354797, 19.819763137656725, 19.819973131843707, 19.819822546049824, 19.820209164862028, 19.82069752265688, 19.820657268429393, 19.820889322109828, 19.8204610535019, 18.109039115859872, 18.355513762397138, 17.883304388834016, 17.995668661910067, 17.814991552798702, 38.189861470504106, 49.315550327440405, 46.83607148260767, 34.540432854960464, 35.316953796472895, 409.09094466734905, 418.07350981867876, 101.92797971359609, 58.26177815399393, 73.76418111436965, 194.51624475428093, 34.84514763620516], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.0619, -5.4627, -5.5625, -5.5875, -5.6169, -5.6444, -5.7666, -5.7752, -5.7752, -5.8072, -5.8785, -5.7607, -4.9387, -5.8337, -5.6681, -5.9309, -5.9383, -5.9354, -5.945, -5.9648, -5.9612, -5.973, -5.9728, -5.9938, -5.9938, -5.9938, -5.9938, -5.9938, -5.9938, -5.9938, -5.9937, -5.0621, -4.8357, -5.0342, -4.8427, -5.3458, -4.4804, -4.3772, -5.3225, -5.1466, -5.2489, -5.3416, -5.0139, -4.7179, -4.9965, -5.6045, -5.1632, -4.2038, -4.234, -5.2394, -4.9183, -5.3689, -5.5222, -5.3183, -5.2649, -5.0543, -5.3103, -5.4652, -4.7792, -4.8897, -5.4128, -5.5167, -5.6321, -5.6632, -5.7139, -5.7438, -5.772, -5.7775, -5.7775, -5.7992, -5.8074, -5.8103, -5.8101, -5.8356, -5.8516, -5.868, -5.8311, -5.8967, -5.8967, -5.9014, -5.9052, -5.9089, -5.9089, -5.9236, -5.9203, -5.9422, -5.935, -5.8896, -4.9703, -4.0875, -4.0769, -4.7915, -5.0153, -5.0638, -5.574, -5.092, -5.3986, -5.2207, -5.2902, -4.3419, -5.0633, -4.8609, -4.6269, -4.9882, -5.0612, -4.5077, -5.2147, -4.5086, -5.6514, -5.1375, -5.6171, -5.615, -5.5356, -3.657, -3.5802, -3.9612, -3.9681, -3.9669, -3.9681, -3.9864, -3.9864, -3.9864, -3.9864, -3.9762, -4.904, -4.9356, -4.9695, -4.9648, -4.9751, -4.9752, -4.9752, -4.9752, -4.9424, -5.1563, -5.1699, -5.1849, -5.1626, -5.1889, -5.1905, -5.1993, -5.1993, -5.1976, -5.1976, -4.0285, -5.1976, -4.0244, -4.911, -4.9998, -4.2683, -3.7666, -3.8781, -4.7043, -4.9258, -4.9538, -3.6283, -3.6376, -3.6376, -3.9393, -3.9551, -3.9572, -3.9586, -3.9611, -3.9591, -3.9611, -3.9611, -3.9636, -4.871, -4.8944, -4.9038, -4.906, -4.9098, -4.9098, -4.9098, -4.9098, -4.9098, -4.9098, -4.9652, -4.9772, -4.9805, -4.9915, -4.9914, -4.9915, -4.9915, -4.9915, -3.9498, -3.9356, -4.0106, -3.7784, -3.5265, -3.4751, -3.5083, -4.8831, -4.8051, -4.4116, -4.4214, -4.6584, -4.7322, -4.7206, -4.7624, -4.7611, -4.7697, -4.755, -4.7795, -4.781, -4.7848, -4.7899, -4.7899, -4.7928, -4.7928, -4.7928, -4.7947, -4.8018, -4.8018, -4.8018, -4.8018, -4.8018, -4.8018, -4.8018, -4.8018, -4.8018, -4.8018, -4.8018, -4.8018, -4.4038, -4.3431, -4.3431, -4.3055, -4.4533, -4.685, -4.5102, -4.6137, -4.5254, -4.5859, -4.7894, -4.7894, -4.7503, -4.7507, -4.7902, -4.7974, -4.7974, -4.7974, -4.7974, -4.9087, -4.9193, -4.9247, -4.9943, -5.0085, -5.1129, -5.169, -5.1932, -5.2082, -5.2082, -5.2082, -5.2082, -5.2082, -5.2082, -5.2082, -5.2082, -5.2082, -5.2082, -5.3025, -5.2891, -5.3154, -5.3092, -5.3195, -4.6767, -4.513, -4.5864, -4.9051, -4.925, -4.4531, -4.4999, -4.8259, -4.9756, -4.9967, -4.8662, -5.2035], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.6518, 1.6387, 1.6352, 1.6336, 1.6325, 1.6315, 1.6255, 1.6251, 1.625, 1.6219, 1.6187, 1.6171, 1.6169, 1.6167, 1.6161, 1.616, 1.6155, 1.6151, 1.6146, 1.6137, 1.6137, 1.6135, 1.6134, 1.6122, 1.6122, 1.6122, 1.6122, 1.6122, 1.6122, 1.6121, 1.6121, 1.5173, 1.4789, 1.4535, 1.4066, 1.4606, 1.2417, 1.199, 1.4424, 1.2865, 1.3243, 1.3347, 1.1451, 0.952, 1.0041, 1.3753, 0.963, -0.1292, -0.1376, 0.9683, 0.5677, 1.0761, 1.2581, 0.9006, 0.6842, -0.1377, 0.3785, 0.4967, 1.7011, 1.6996, 1.6837, 1.6796, 1.6743, 1.6722, 1.67, 1.6687, 1.667, 1.6666, 1.6665, 1.6658, 1.6653, 1.6649, 1.6647, 1.6637, 1.6627, 1.6613, 1.6611, 1.66, 1.66, 1.6597, 1.6594, 1.6593, 1.6592, 1.658, 1.6574, 1.6572, 1.6572, 1.6566, 1.6159, 1.436, 1.4285, 1.4717, 1.4514, 1.4362, 1.5434, 1.3591, 1.4338, 1.3473, 1.2858, 0.5747, 0.9821, 0.7153, 0.2129, 0.647, 0.661, -0.4113, 0.7357, -0.434, 1.365, 0.0843, 1.0395, 0.9868, 0.2739, 1.7284, 1.7268, 1.7262, 1.7262, 1.7262, 1.7262, 1.726, 1.726, 1.726, 1.726, 1.7243, 1.7117, 1.7109, 1.71, 1.7099, 1.7099, 1.7099, 1.7099, 1.7099, 1.7096, 1.7044, 1.7042, 1.7037, 1.7037, 1.7037, 1.7036, 1.7035, 1.7035, 1.7034, 1.7034, 1.6319, 1.7034, 1.5995, 1.5578, 1.5943, 0.9535, 0.308, 0.2183, 0.9845, 1.2498, 1.2579, 1.7922, 1.7922, 1.7922, 1.7895, 1.7894, 1.7894, 1.7894, 1.7894, 1.7894, 1.7894, 1.7893, 1.7893, 1.7744, 1.7739, 1.7737, 1.7736, 1.7736, 1.7736, 1.7736, 1.7736, 1.7735, 1.7735, 1.772, 1.7718, 1.7717, 1.7714, 1.7714, 1.7714, 1.7714, 1.7714, 1.7709, 1.7552, 1.6066, 1.5354, 1.3133, 0.6213, 0.5664, 1.6688, 0.1115, 1.8673, 1.8671, 1.8614, 1.861, 1.8608, 1.8603, 1.8603, 1.8601, 1.86, 1.8598, 1.8598, 1.8597, 1.8596, 1.8596, 1.8596, 1.8596, 1.8596, 1.8595, 1.8594, 1.8594, 1.8594, 1.8594, 1.8593, 1.8593, 1.8593, 1.8593, 1.8593, 1.8593, 1.8593, 1.8593, 1.7449, 1.6826, 1.6826, 1.5516, 1.5308, 1.7121, 1.4517, 1.4927, -0.4508, -0.4896, 1.5645, 1.5645, 1.9301, 1.93, 1.9291, 1.9289, 1.9289, 1.9289, 1.9289, 1.9255, 1.9254, 1.925, 1.9232, 1.9219, 1.919, 1.917, 1.916, 1.9155, 1.9155, 1.9155, 1.9154, 1.9154, 1.9154, 1.9154, 1.9154, 1.9154, 1.9154, 1.9114, 1.9113, 1.911, 1.911, 1.9108, 1.7911, 1.699, 1.6773, 1.6631, 1.621, -0.3567, -0.4253, 0.6601, 1.0698, 0.8128, -0.0264, 1.3559]}, \"token.table\": {\"Topic\": [1, 4, 6, 6, 5, 5, 2, 3, 4, 4, 3, 1, 3, 1, 5, 5, 2, 2, 6, 1, 5, 6, 1, 6, 6, 6, 3, 3, 4, 4, 4, 1, 1, 2, 4, 5, 6, 1, 2, 4, 5, 6, 3, 3, 2, 5, 5, 5, 2, 2, 4, 4, 5, 5, 2, 3, 6, 6, 2, 6, 6, 1, 3, 5, 3, 1, 1, 2, 4, 5, 4, 2, 2, 1, 4, 5, 1, 1, 2, 4, 6, 2, 3, 3, 5, 1, 2, 3, 5, 6, 5, 3, 3, 3, 5, 5, 1, 5, 5, 6, 2, 2, 4, 4, 2, 5, 3, 1, 2, 3, 4, 5, 6, 5, 2, 6, 4, 5, 1, 2, 6, 2, 6, 6, 2, 1, 2, 4, 1, 2, 5, 6, 2, 6, 4, 5, 5, 2, 1, 2, 2, 6, 1, 1, 1, 4, 4, 1, 5, 5, 4, 4, 4, 4, 1, 1, 1, 1, 1, 3, 6, 4, 5, 4, 5, 2, 2, 1, 2, 3, 1, 2, 4, 1, 2, 6, 2, 1, 2, 5, 1, 1, 2, 3, 5, 6, 3, 2, 4, 5, 6, 4, 1, 2, 6, 1, 2, 3, 5, 3, 6, 1, 6, 2, 1, 2, 3, 3, 6, 6, 1, 2, 2, 2, 5, 3, 3, 1, 2, 4, 5, 5, 1, 1, 2, 3, 6, 6, 1, 2, 3, 5, 6, 4, 3, 3, 6, 4, 1, 2, 6, 2, 5, 5, 4, 5, 4, 5, 1, 1, 5, 5, 2, 2, 5, 6, 6, 2, 1, 2, 6, 2, 6, 2, 6, 1, 4, 6, 1, 1, 2, 6, 1, 6, 4, 6, 6, 1, 2, 4, 5, 6, 6, 4, 4, 6, 5, 3, 3, 4, 3, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 3, 3, 1, 1, 2, 1, 2, 5, 6, 1, 2, 5, 6, 1, 2, 3, 4, 6, 2, 4, 4, 2, 3, 2, 3, 2, 4, 4, 1, 1, 2, 3, 3, 5, 5, 1, 1, 1, 2, 3, 2, 1, 2, 1, 6, 1, 1, 2, 6, 1, 2, 6, 6, 4, 4, 2, 3, 5, 2, 1, 2, 5, 6, 1, 2, 3, 5, 6, 1, 2, 6, 1, 2, 3, 4, 5, 1, 2, 4, 5, 6, 5, 1, 2, 3, 3, 1, 2, 5, 6, 1, 2, 6, 1, 2, 4, 5, 6, 6, 1, 2, 3, 4, 5, 6, 2, 1, 1, 2, 6, 1, 1, 1, 2, 4, 5, 6, 6], \"Freq\": [0.9238162905604533, 0.9923410895551731, 0.9718167904129602, 0.9833905992858817, 0.9850045634576251, 0.9783309839424943, 0.9482178728916332, 0.9842210093430246, 0.9880816858301112, 0.9709428423160407, 0.9936276566749217, 0.11883780495288276, 0.8615740859084, 0.14639388763888322, 0.8539643445601521, 0.9849930402987626, 0.9238470185348263, 0.9163460507690142, 0.9542524872726831, 0.3741279560812195, 0.3741279560812195, 0.24941863738747969, 0.9780087936169823, 0.9585939131698697, 0.9720385586792687, 0.9833625689166947, 0.9759854784741542, 0.9762142076643048, 0.9745435479333412, 0.9688677405530289, 0.9745533628177472, 0.9615909782169352, 0.48344016717394306, 0.16507713025451715, 0.023582447179216735, 0.25940691897138407, 0.07074734153765021, 0.4838430717362966, 0.14887479130347586, 0.037218697825868965, 0.2419215358681483, 0.07443739565173793, 0.9895146057102668, 0.9842155845615499, 0.8843196116803723, 0.08843196116803723, 0.9763134620791043, 0.976307186018312, 0.973111611272051, 0.9731761670829373, 0.9923719457696574, 0.9923326606666638, 0.965148039051333, 0.9735930944507176, 0.02831501283386083, 0.25483511550474747, 0.7078753208465207, 0.958584637209283, 0.9571516123911179, 0.9716806531647597, 0.9833609697399824, 0.9541747808278853, 0.9846783590860437, 0.9849651888112698, 0.9662363541452405, 0.9308962490211627, 0.12770988348071705, 0.07297707627469545, 0.12770988348071705, 0.6750379555409329, 0.9848480978022991, 0.9335901212136671, 0.9335827042831321, 0.1403838615973386, 0.7679823016795582, 0.09083661632768968, 0.8989872684475704, 0.44916872464517693, 0.08166704084457763, 0.38791844401174375, 0.06125028063343322, 0.9534177584398359, 0.9697398453431432, 0.9691929336261497, 0.9687307685783849, 0.05272646912877236, 0.1958411710497259, 0.45947351669358766, 0.20337352378240767, 0.09038823279218118, 0.9636946499890521, 0.9959953386489313, 0.9842224038313976, 0.9842253637382499, 0.9849568696325733, 0.9849584627020096, 0.9507309319453757, 0.968203275783582, 0.9849587539057896, 0.9586749901958902, 0.9318930775459315, 0.024810454025568328, 0.9676077069971648, 0.9909735623019577, 0.9694561915410904, 0.9781296809870328, 0.9828247408902422, 0.16098190378442698, 0.3164127074383565, 0.0943687022184572, 0.1831863043064169, 0.16653300391492445, 0.07216430169646727, 0.9849484308224838, 0.9084089598968267, 0.9806317727196806, 0.9599091749594967, 0.9755544573887105, 0.2582856038942056, 0.20088880302882656, 0.5452696082211006, 0.22305337620615226, 0.7705480268939805, 0.9833720577735867, 0.9443605952203235, 0.08753717134694375, 0.6419392565442541, 0.2626115140408312, 0.06865564572072358, 0.48058952004506506, 0.051491734290542684, 0.4119338743243415, 0.9299584617927875, 0.9449322213619341, 0.9745431901676024, 0.9735904001062768, 0.9849580512245173, 0.9691278647957414, 0.9434395510062177, 0.02858907730321872, 0.13092480065322426, 0.8379187241806353, 0.9107810944931236, 0.936332453556139, 0.11393477388097528, 0.8829944975775584, 0.9745307510974454, 0.8989806799360524, 0.965517124180249, 0.9849586886175895, 0.9743464236299777, 0.974549088736461, 0.9929796462890434, 0.9841086697131659, 0.9314275423373917, 0.8990083244921657, 0.9314529015622454, 0.8990015097715534, 0.7428247152217016, 0.17478228593451803, 0.08739114296725901, 0.2570408252635789, 0.7243877802882679, 0.2570440641548203, 0.7243969080726753, 0.9757942779267078, 0.9743918200459629, 0.7576747430180026, 0.18941868575450066, 0.04209304127877792, 0.899015519925743, 0.0361214402319573, 0.9511979261082087, 0.10463598826824869, 0.7586109149448029, 0.13079498533531086, 0.9682450580321502, 0.6829021983091211, 0.22763406610304038, 0.07587802203434679, 0.8990557692879396, 0.276361702773475, 0.18023589311313587, 0.46861332209415324, 0.03604717862262717, 0.04806290483016956, 0.9762089704663639, 0.022369794055898652, 0.8276823800682501, 0.13421876433539193, 0.011184897027949326, 0.9841046825876303, 0.18853235527922216, 0.7541294211168886, 0.03770647105584443, 0.17596651840102168, 0.13686284764523907, 0.6061068967146302, 0.05865550613367389, 0.9762054202127428, 0.9506073167671667, 0.9245384534854141, 0.9387577049911733, 0.9227477218558207, 0.15042114823514943, 0.8523865066658468, 0.9751427580798557, 0.9709949538426337, 0.966676837970082, 0.9586053497298973, 0.21341011155582876, 0.7682764016009835, 0.9323154583924438, 0.5060459656826747, 0.4744180928275075, 0.9625092205366469, 0.9692137938704157, 0.04842898058213948, 0.08071496763689913, 0.16142993527379826, 0.7102917152047123, 0.984940024033315, 0.9710888992947024, 0.028951576959070642, 0.11580630783628257, 0.08685473087721193, 0.7527410009358367, 0.9586289483649048, 0.508809201431117, 0.19695840055398076, 0.08206600023082532, 0.03282640009233013, 0.19695840055398076, 0.9848386958485076, 0.9572676428087574, 0.9692216787463048, 0.9446717607099804, 0.9848606410335344, 0.9479125335578066, 0.9352844850529392, 0.9852869587421674, 0.9810466221312377, 0.9770718645321912, 0.9849550163553102, 0.16829635635248555, 0.8246521461271792, 0.1682963659562022, 0.8246521931853908, 0.9375534176303275, 0.11419144317829055, 0.8754677310335609, 0.9849622012197979, 0.9377139964741931, 0.12810638915836628, 0.10675532429863857, 0.74728727009047, 0.9565180774191534, 0.9821198757700947, 0.7096746173160174, 0.25806349720582455, 0.958639105222241, 0.6953156390086659, 0.28327674181834533, 0.7572678629929128, 0.2434075273905791, 0.8088329378898212, 0.13480548964830355, 0.02246758160805059, 0.9400278010209135, 0.08684621265311611, 0.6947697012249289, 0.23158990040830962, 0.8081903083112048, 0.18367961552527381, 0.9848442634317569, 0.9820953568950301, 0.9542824738304017, 0.10845372210661684, 0.2304641594765608, 0.31180445105652344, 0.02711343052665421, 0.3253611663198505, 0.958617532335828, 0.9600235985120784, 0.9848412412886507, 0.9590786476824624, 0.9849769031641885, 0.9705831914309063, 0.9708189322863083, 0.9713279414538611, 0.9907909024422417, 0.9907825334232976, 0.16504274578393105, 0.11481234489316942, 0.23919238519410296, 0.29181470993680564, 0.0956769540776412, 0.09328503022570016, 0.16133331930304076, 0.11733332312948418, 0.21999998086778283, 0.307999973214896, 0.09288888081084165, 0.1002222135064344, 0.9610569558100016, 0.9709501171400589, 0.899019612081522, 0.6460160807563365, 0.34785481271495045, 0.15004532635624246, 0.7402236100241294, 0.04001208702833132, 0.07002115229957981, 0.1425958663154176, 0.7537210076672074, 0.04074167609011932, 0.0712979331577088, 0.020563835195630712, 0.22106122835303016, 0.005140958798907678, 0.6169150558689214, 0.1388058875705073, 0.9549407543894888, 0.9865495351313809, 0.9948263295368016, 0.12386532394251273, 0.867057267597589, 0.09343792179979195, 0.8993399973229975, 0.9673361475369179, 0.9696186082974843, 0.9745625585553538, 0.9587244120090653, 0.9093977001700739, 0.15728106602842973, 0.8388323521516252, 0.9714781115508533, 0.9853343599515908, 0.9763192319272304, 0.9278642685572606, 0.9726438983480112, 0.6227459107647759, 0.3650579476896962, 0.010736998461461653, 0.9444275540416011, 0.7753993636714341, 0.1762271281071441, 0.5375125878342532, 0.43512923777058593, 0.9529289720196921, 0.6443094275649046, 0.21476980918830152, 0.143179872792201, 0.8013202854915836, 0.18697473328136952, 0.9585958600002359, 0.958636231775283, 0.9944023479884647, 0.9840978910014375, 0.9429683739487404, 0.9925064048027998, 0.9708983036889541, 0.9766587909252815, 0.6983327343478222, 0.20368038085144813, 0.05819439452898518, 0.02909719726449259, 0.2653292058215995, 0.3745824082187288, 0.12486080273957625, 0.0468228010273411, 0.1872912041093644, 0.27195461332613796, 0.6798865333153449, 0.04532576888768966, 0.5768756347140656, 0.07691675129520874, 0.019229187823802184, 0.03845837564760437, 0.2692086295332306, 0.028437323245679862, 0.09953063135987952, 0.14218661622839932, 0.7109330811419966, 0.014218661622839931, 0.9795936622377209, 0.26352497852229506, 0.10135576097011348, 0.6284057180147036, 0.9761919686433473, 0.30000802802568555, 0.015789896211878186, 0.6473857446870056, 0.015789896211878186, 0.1497069629192004, 0.47906228134144124, 0.35929671100608096, 0.33356886004741215, 0.09810848824923887, 0.06867594177446722, 0.2354603717981733, 0.27470376709786887, 0.9764942510654506, 0.19362187188815377, 0.34168565627321257, 0.04555808750309501, 0.25056948126702255, 0.14806378438505877, 0.011389521875773752, 0.941041276994867, 0.9455270143232865, 0.8490495232926123, 0.05855513953742154, 0.05855513953742154, 0.8989447474736402, 0.9617576667890186, 0.4845578899027209, 0.08075964831712015, 0.020189912079280038, 0.08075964831712015, 0.3230385932684806, 0.9586531757403652], \"Term\": [\"abrams american\", \"abrams carrying\", \"abrams cheated\", \"abrams consequential\", \"abrams goddess\", \"abrams raphael\", \"abrams said\", \"abrams spun\", \"abrams state\", \"abrams super\", \"abrams tweet\", \"abrams way\", \"abrams way\", \"absolutely\", \"absolutely\", \"absolutely quite\", \"activist\", \"actually\", \"air\", \"american\", \"american\", \"american\", \"american hero\", \"american royalty\", \"argument\", \"argument stacey\", \"asking\", \"asking talk\", \"believe happen\", \"believing\", \"believing stacey\", \"biden victory\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black woman\", \"black woman\", \"black woman\", \"black woman\", \"black woman\", \"block\", \"block em\", \"blue\", \"blue\", \"boiled\", \"boiled egg\", \"brian\", \"brian kemp\", \"carrying\", \"carrying democracy\", \"cat\", \"cat happiness\", \"cheated\", \"cheated\", \"cheated\", \"cheated governor\", \"clearly\", \"consequential\", \"consequential woman\", \"covid\", \"crazy\", \"cup\", \"dad\", \"date\", \"day\", \"day\", \"day\", \"day\", \"decade building\", \"declined\", \"declined run\", \"democracy\", \"democracy\", \"democracy\", \"democracy real\", \"democrat\", \"democrat\", \"democrat\", \"democrat\", \"deserves\", \"determined\", \"determined stacey\", \"egg\", \"election\", \"election\", \"election\", \"election\", \"election\", \"election georgia\", \"election stacey\", \"em\", \"em crazy\", \"englishman\", \"englishman uk\", \"entire\", \"far\", \"far important\", \"flipped motherfucking\", \"focus\", \"follow\", \"follow\", \"follow lead\", \"followed\", \"frankly\", \"fucking abrams\", \"georgia\", \"georgia\", \"georgia\", \"georgia\", \"georgia\", \"georgia\", \"georgia absolutely\", \"georgia blue\", \"georgia republican\", \"georgia senate\", \"goddess\", \"going\", \"going\", \"going\", \"good\", \"good\", \"good argument\", \"goodness\", \"got\", \"got\", \"got\", \"governor\", \"governor\", \"governor\", \"governor\", \"governor kemp\", \"governor year\", \"happen work\", \"happiness\", \"happiness far\", \"harris\", \"hero\", \"hero\", \"history\", \"history\", \"house\", \"huge\", \"human\", \"human\", \"human believe\", \"imaginary world\", \"important\", \"important stacey\", \"infrastructure\", \"instead believing\", \"invest\", \"invest follow\", \"jarpad\", \"jarpad stacey\", \"jensenackles\", \"jensenackles jarpad\", \"joe\", \"joe\", \"joe\", \"jon\", \"jon\", \"jon ossoff\", \"jon ossoff\", \"kemp\", \"kemp stole\", \"know\", \"know\", \"know\", \"know date\", \"lead\", \"lead\", \"leader\", \"leader\", \"leader\", \"lesson\", \"let\", \"let\", \"let\", \"let know\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like dad\", \"listen\", \"listen\", \"listen\", \"listen\", \"listen trust\", \"lot\", \"lot\", \"lot\", \"make\", \"make\", \"make\", \"make\", \"make sound\", \"match\", \"maybe\", \"meaningful\", \"mobilization\", \"moment\", \"moment\", \"mother\", \"mother abrams\", \"motherfucking\", \"motherfucking senate\", \"need\", \"need\", \"needed\", \"office\", \"office\", \"optimistic\", \"optimistic pessimistic\", \"ossoff\", \"ossoff\", \"ossoff\", \"ossoff\", \"ossoff really\", \"owe\", \"party\", \"party\", \"party\", \"party\", \"party flipped\", \"people\", \"people\", \"people\", \"people\", \"people\", \"person responsible\", \"pessimistic\", \"pessimistic determined\", \"phone\", \"political landscape\", \"politician\", \"politics\", \"pundit\", \"queen\", \"quite\", \"quite frankly\", \"raphael\", \"raphael\", \"raphael warnock\", \"raphael warnock\", \"real\", \"really\", \"really\", \"really day\", \"reform\", \"republican\", \"republican\", \"republican\", \"republican party\", \"response\", \"right\", \"right\", \"royalty\", \"run\", \"run\", \"said\", \"said\", \"saved\", \"saved\", \"saved\", \"saved democracy\", \"saving\", \"saving\", \"saving\", \"say\", \"say\", \"seat person\", \"secretary\", \"secretary state\", \"senate\", \"senate\", \"senate\", \"senate\", \"senate\", \"senate american\", \"senate seat\", \"shifting\", \"signature\", \"soft boiled\", \"sound\", \"sound like\", \"spent decade\", \"spun\", \"spun block\", \"stacey\", \"stacey\", \"stacey\", \"stacey\", \"stacey\", \"stacey\", \"stacey abrams\", \"stacey abrams\", \"stacey abrams\", \"stacey abrams\", \"stacey abrams\", \"stacey abrams\", \"stacey fucking\", \"stacey mother\", \"stacey saved\", \"staceyabrams\", \"staceyabrams\", \"stacy\", \"stacy\", \"stacy\", \"stacy\", \"stacy abrams\", \"stacy abrams\", \"stacy abrams\", \"stacy abrams\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state blue\", \"state listen\", \"state stacey\", \"stole\", \"stole\", \"stole election\", \"stole election\", \"stolen\", \"super\", \"super human\", \"supporter\", \"sure\", \"talk\", \"talk\", \"talk stacey\", \"tea\", \"tea cat\", \"team\", \"tell\", \"thank\", \"thank\", \"thank\", \"thank goodness\", \"thank stacey\", \"thank stacey\", \"thing\", \"thing\", \"thing know\", \"think\", \"think\", \"think\", \"time\", \"time\", \"torched\", \"torched republican\", \"trust\", \"trust invest\", \"turned\", \"tweet\", \"uk\", \"vision\", \"vote\", \"vote\", \"vote\", \"vote\", \"voter\", \"voter\", \"voter\", \"voter\", \"voter\", \"voting\", \"voting\", \"voting\", \"want\", \"want\", \"want\", \"want\", \"want\", \"warnock\", \"warnock\", \"warnock\", \"warnock\", \"warnock\", \"warnock jon\", \"way\", \"way\", \"way\", \"way make\", \"white\", \"white\", \"white\", \"white\", \"win\", \"win\", \"win\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman history\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work georgia\", \"worked\", \"world\", \"world\", \"world\", \"world saved\", \"world thank\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year torched\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [6, 5, 3, 4, 1, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el35111402784079817765761016317\", ldavis_el35111402784079817765761016317_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el35111402784079817765761016317\", ldavis_el35111402784079817765761016317_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el35111402784079817765761016317\", ldavis_el35111402784079817765761016317_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=               x          y  topics  cluster       Freq\n",
       "topic                                                  \n",
       "5      1.465019  -47.625065  1       1        18.647875\n",
       "4      54.854797 -36.476795  2       1        17.844918\n",
       "2      32.212929 -61.444572  3       1        17.625091\n",
       "3      5.111096  -14.110332  4       1        16.537567\n",
       "0      26.350170 -33.374702  5       1        15.195713\n",
       "1      38.109818 -7.222781   6       1        14.148837, topic_info=                    Term        Freq       Total Category  logprob  loglift\n",
       "1800  tweet               121.000000  121.000000  Default  30.0000  30.0000\n",
       "306   carrying            108.000000  108.000000  Default  29.0000  29.0000\n",
       "87    abrams tweet        112.000000  112.000000  Default  28.0000  28.0000\n",
       "13    abrams carrying     107.000000  107.000000  Default  27.0000  27.0000\n",
       "307   carrying democracy  107.000000  107.000000  Default  26.0000  26.0000\n",
       "...                  ...         ...         ...      ...      ...      ...\n",
       "1931  woman               27.906480   101.927980  Topic6  -4.8259   0.6601 \n",
       "753   governor            24.027392   58.261778   Topic6  -4.9756   1.0698 \n",
       "1535  senate              23.525733   73.764181   Topic6  -4.9967   0.8128 \n",
       "1647  state               26.803888   194.516245  Topic6  -4.8662  -0.0264 \n",
       "731   going               19.129909   34.845148   Topic6  -5.2035   1.3559 \n",
       "\n",
       "[307 rows x 6 columns], token_table=      Topic      Freq                  Term\n",
       "term                                       \n",
       "5     1      0.923816  abrams american     \n",
       "13    4      0.992341  abrams carrying     \n",
       "15    6      0.971817  abrams cheated      \n",
       "17    6      0.983391  abrams consequential\n",
       "33    5      0.985005  abrams goddess      \n",
       "...  ..           ...                   ...\n",
       "1985  2      0.080760  year                \n",
       "1985  4      0.020190  year                \n",
       "1985  5      0.080760  year                \n",
       "1985  6      0.323039  year                \n",
       "1990  6      0.958653  year torched        \n",
       "\n",
       "[422 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[6, 5, 3, 4, 1, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualization of LDA model \n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda_model, doc_term_matrix_1, tfidfconverter, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LatentDirichletAllocation(),\n",
       "             param_grid={'learning_decay': [0.5, 0.7, 0.9],\n",
       "                         'n_components': [5, 8, 10, 12]})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross-validation to find best parameters for LDA model\n",
    "\n",
    "#import cross-validation tool\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define search parameters\n",
    "search_params = {'n_components': [5, 8, 10, 12], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# initialize model for cross-validation\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# initialize grid search class \n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# run grid search \n",
    "model.fit(doc_term_matrix_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for best model:  {'learning_decay': 0.7, 'n_components': 5}\n",
      "Log likelihood score of best model:  -39634.21593541675\n",
      "Perplexity score of best model:  1027.9415668034726\n"
     ]
    }
   ],
   "source": [
    "# what is best model? \n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# print parameters for best model\n",
    "print(\"Parameters for best model: \", model.best_params_)\n",
    "\n",
    "# print log likelihood score\n",
    "print(\"Log likelihood score of best model: \", model.best_score_)\n",
    "\n",
    "# print perplexity score\n",
    "print(\"Perplexity score of best model: \", best_lda_model.perplexity(doc_term_matrix_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Model:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lda_model_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-201d351295f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LDA Model:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidfconverter\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lda_model_2' is not defined"
     ]
    }
   ],
   "source": [
    "# top topics of 2nd LDA model\n",
    "\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]])\n",
    " \n",
    "print(\"LDA Model:\")\n",
    "print_topics(lda_model_2, tfidfconverter )\n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd LDA model: running new \"best model\" parameters\n",
    "\n",
    "lda_model_2 = LatentDirichletAllocation(n_components = 5, max_iter=10, learning_method='online', learning_decay=0.5)\n",
    "lda_Z_2 = lda_model_2.fit_transform(doc_term_matrix_1)\n",
    "\n",
    "# visualization of 2nd LDA model \n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda_model_2, doc_term_matrix_1, tfidfconverter, mds='tsne')\n",
    "panel\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda_model_2, doc_term_matrix_1, tfidfconverter, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Additional data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert NMF model, LDA model 1, and LDA model 2 results back into dataframe\n",
    "topic_values = nmf_model.transform(doc_term_matrix_1)\n",
    "df['NMF_topic'] = topic_values.argmax(axis=1)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_values = lda_model.transform(doc_term_matrix_1)\n",
    "df['LDA1_topic'] = topic_values.argmax(axis=1)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_values = lda_model_2.transform(doc_term_matrix_1)\n",
    "df['LDA2_topic'] = topic_values.argmax(axis=1)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "df = df.drop(['id', 'parsed_created_at', 'user_screen_name', 'text',\n",
    "       'hashtags', 'user_id', 'processed_text', 'pos_tagged', 'lemmatized', 'final_docs'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
