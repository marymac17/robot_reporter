{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import initial libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#import initial libraries\n",
    "\n",
    "df = pd.read_csv('data/coup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 413600 entries, 0 to 413599\n",
      "Data columns (total 37 columns):\n",
      " #   Column                        Non-Null Count   Dtype  \n",
      "---  ------                        --------------   -----  \n",
      " 0   id                            413600 non-null  int64  \n",
      " 1   tweet_url                     413600 non-null  object \n",
      " 2   created_at                    413600 non-null  object \n",
      " 3   parsed_created_at             413600 non-null  object \n",
      " 4   user_screen_name              413600 non-null  object \n",
      " 5   text                          413600 non-null  object \n",
      " 6   tweet_type                    413600 non-null  object \n",
      " 7   coordinates                   7 non-null       object \n",
      " 8   hashtags                      13057 non-null   object \n",
      " 9   media                         11642 non-null   object \n",
      " 10  urls                          52890 non-null   object \n",
      " 11  favorite_count                413600 non-null  int64  \n",
      " 12  in_reply_to_screen_name       31057 non-null   object \n",
      " 13  in_reply_to_status_id         27532 non-null   float64\n",
      " 14  in_reply_to_user_id           31057 non-null   float64\n",
      " 15  lang                          413600 non-null  object \n",
      " 16  place                         1702 non-null    object \n",
      " 17  possibly_sensitive            39368 non-null   object \n",
      " 18  retweet_count                 413600 non-null  int64  \n",
      " 19  retweet_or_quote_id           366954 non-null  float64\n",
      " 20  retweet_or_quote_screen_name  366954 non-null  object \n",
      " 21  retweet_or_quote_user_id      366954 non-null  float64\n",
      " 22  source                        413566 non-null  object \n",
      " 23  user_id                       413600 non-null  int64  \n",
      " 24  user_created_at               413600 non-null  object \n",
      " 25  user_default_profile_image    413600 non-null  bool   \n",
      " 26  user_description              350206 non-null  object \n",
      " 27  user_favourites_count         413600 non-null  int64  \n",
      " 28  user_followers_count          413600 non-null  int64  \n",
      " 29  user_friends_count            413600 non-null  int64  \n",
      " 30  user_listed_count             413600 non-null  int64  \n",
      " 31  user_location                 274043 non-null  object \n",
      " 32  user_name                     413574 non-null  object \n",
      " 33  user_statuses_count           413600 non-null  int64  \n",
      " 34  user_time_zone                0 non-null       float64\n",
      " 35  user_urls                     0 non-null       float64\n",
      " 36  user_verified                 413600 non-null  bool   \n",
      "dtypes: bool(2), float64(6), int64(9), object(20)\n",
      "memory usage: 111.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# look at basic info about data\n",
    "\n",
    "df.info()\n",
    "# this data set consists of 413,600 Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 ['en' 'und' 'fr' 'de' 'nl' 'in' 'th' 'ja' 'pt' 'es' 'et' 'ca' 'fa' 'it'\n",
      " 'tr' 'pl' 'ht' 'cy' 'ko' 'ro' 'tl' 'eu' 'zh' 'ar' 'no' 'ru' 'fi' 'pa'\n",
      " 'el' 'vi' 'sv' 'iw' 'sr' 'ur' 'hu' 'lt' 'cs' 'lv' 'da' 'hi' 'is' 'sl'\n",
      " 'ta' 'dv']\n"
     ]
    }
   ],
   "source": [
    "# count number of languages in data set\n",
    "\n",
    "count_lang = df['lang'].unique()\n",
    "print(len(count_lang), count_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(396586, 37)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweets are in 44 different languages\n",
    "\n",
    "# I'll be working only with tweets in English\n",
    "# drop tweets in all other languages\n",
    "# now working with 396,586 Tweets \n",
    "\n",
    "df = df[df.lang == 'en']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns \n",
    "df = df.drop(['tweet_url', 'created_at', 'media', 'urls','in_reply_to_screen_name',\n",
    "       'in_reply_to_status_id', 'in_reply_to_user_id', 'retweet_or_quote_id',\n",
    "       'retweet_or_quote_screen_name', 'retweet_or_quote_user_id', 'source',\n",
    "       'user_created_at', 'user_name', 'user_verified', 'user_friends_count', 'user_listed_count',\n",
    "       'user_statuses_count', 'user_default_profile_image', 'user_description',\n",
    "       'user_favourites_count', 'user_followers_count', 'coordinates', 'lang'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                  1346974253178970113\n",
       "parsed_created_at                             2021-01-07 00:17:42+00:00\n",
       "user_screen_name                                               faby1717\n",
       "text                  Assault on democracy: Sen. Josh Hawley has blo...\n",
       "tweet_type                                                      retweet\n",
       "hashtags                                                            NaN\n",
       "favorite_count                                                     7579\n",
       "place                                                               NaN\n",
       "possibly_sensitive                                                False\n",
       "retweet_count                                                         0\n",
       "user_id                                                       115280140\n",
       "user_location                                                 CA Desert\n",
       "user_time_zone                                                      NaN\n",
       "user_urls                                                           NaN\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check start time & date of data\n",
    "\n",
    "df.iloc[0]\n",
    "\n",
    "# first Tweet downloaded Jan 7, 2021 at 00:17:42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                  1347018669625286656\n",
       "parsed_created_at                             2021-01-07 03:14:11+00:00\n",
       "user_screen_name                                                kt_dinh\n",
       "text                  Six Senators voted to sustain the objection to...\n",
       "tweet_type                                                      retweet\n",
       "hashtags                                                            NaN\n",
       "favorite_count                                                        6\n",
       "place                                                               NaN\n",
       "possibly_sensitive                                                  NaN\n",
       "retweet_count                                                         0\n",
       "user_id                                             1249340729689686016\n",
       "user_location                                                       NaN\n",
       "user_time_zone                                                      NaN\n",
       "user_urls                                                           NaN\n",
       "Name: 413599, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check end time & date of data \n",
    "\n",
    "df.iloc[-1]\n",
    "\n",
    "# last Tweet on Jan 6, 2021 at 03:14:11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Text processing for NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize, remove stopwords, remove urls, lowercase, remove punctuation, remove numbers\n",
    "# import necessary libraries: ntlk etc.\n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "punc = list(set(string.punctuation))\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def remove_url(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r'', text)\n",
    "\n",
    "def process_text(text):\n",
    "    text = remove_url(text)\n",
    "    text = tokenizer(text)\n",
    "    text = [word.lower() for word in text]\n",
    "    text = [re.sub('[0-9]+', '', word) for word in text]\n",
    "    text = [word for word in text if word not in punc]\n",
    "    text = [word for word in text if word not in stop]\n",
    "    text = [each for each in text if len(each) > 1]\n",
    "    text = [word for word in text if ' ' not in word]\n",
    "     \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply text processing functions to text\n",
    "df['processed_text'] = df['text'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     [assault, democracy, sen, josh, hawley, blood, hands, capitol, coup, attempt]                                                                                                                                              \n",
       "1     [call, old-fashioned, armed, insurgents, breach, capitol, building, request, president, i'd, call, attempted, coup]                                                                                                        \n",
       "2     [@vritrite, @dreddersart, oh, yeah, like, politics, bad, im, like, ignore, coup, attempt, happening]                                                                                                                       \n",
       "3     [ridiculously, divisive, statement, btw, gun, violence, continues, baltimore, oh, still, picked, trash]                                                                                                                    \n",
       "4     [assault, democracy, sen, josh, hawley, blood, hands, capitol, coup, attempt]                                                                                                                                              \n",
       "5     [sort, editorial, home, state, newspaper, politicians, want]                                                                                                                                                               \n",
       "6     [impeach, remove, president, staging, coup, what's, point, impeachment]                                                                                                                                                    \n",
       "7     [middle, insurrection, spurred, father, fever, dream, attempted, coup, maybe, send, brother, text, ...]                                                                                                                    \n",
       "8     [locked, hours, inciting, coup, main]                                                                                                                                                                                      \n",
       "9     [imagine, obama, lost, spent, next, two, months, inciting, coup, refusing, concede, hundreds, black, protesters, stormed, capitol, building, try, stop, certification, defeat, ..., imagine]                               \n",
       "10    [assault, democracy, sen, josh, hawley, blood, hands, capitol, coup, attempt]                                                                                                                                              \n",
       "11    [@cooper_m, cal, brings, antifa, left, tomorrow, need, slap, enough, enough, coup, d'etat]                                                                                                                                 \n",
       "12    [rosenstein, together, pence's, knowledge, consent, went, help, engineer, entire, coup, attempt, president, trump, ..., promote, pence, presidency, point, pence, would, turn, around, appoint, rod, rosenstein, vp]       \n",
       "13    [make, mistake, so-called, leader, country, swore, protect, defend, constitution, directed, horde, white, supremacist, domestic, terrorists, stage, coup, overturn, legitimately, decided, election, encouraging, #traitor]\n",
       "14    [everyone, supported, president's, attempted, coup, today, led, storming, us, capitol, written, public, life, forever, @hawleymo, @tedcruz, via, @tpm]                                                                     \n",
       "15    [republicans, guilty, participating, trump's, coup, everyone, held, accountable, justice, system, @gop, @gopleader, @kamalaharris, @joebiden, @aoc]                                                                        \n",
       "16    [ughbme]                                                                                                                                                                                                                   \n",
       "17    [every, single, person, participated, attempted, coup, must, held, accountable, full, extent, law]                                                                                                                         \n",
       "18    [twitter, pledges, take, action, calls, violence, capitol, locks, @cnbc, wow, took, trump, account, hours, ..., mr, trump, organize, coup, country, hours]                                                                 \n",
       "19    [every, seditionist, house, member, supported, encouraged, attempted, coup, needs, expelled]                                                                                                                               \n",
       "Name: processed_text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at some of processed text\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "df['processed_text'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part-of-speech tagging \n",
    "\n",
    "ready_for_pos = df['processed_text']\n",
    "\n",
    "def pos_tagging(text):\n",
    "    pos_tag = [pos_tag(word) for word in ready_for_pos]\n",
    "\n",
    "df['pos_tagged'] = df.processed_text.apply(lambda x: pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizing\n",
    "\n",
    "pos_tagged = df['pos_tagged']\n",
    "\n",
    "wordnet = WordNetLemmatizer() \n",
    "\n",
    "lemmatized = [[wordnet.lemmatize(word[0]) for word in words] for words in pos_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagged = df['pos_tagged']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizing\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "wordnet = WordNetLemmatizer() \n",
    "\n",
    "lemmatized = [[wordnet.lemmatize(word[0]) for word in words] for words in pos_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at lemmatized text\n",
    "\n",
    "df['lemmatized'] = lemmatized\n",
    "lemmatized[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before vectorizing, cast lists of words back into strings\n",
    "\n",
    "df['final_docs'] = df['lemmatized'].apply(lambda x: \" \".join(x))\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "final_docs = df['final_docs']\n",
    "final_docs[3000:3020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Part 3: run NMF and LDA models, for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'final_docs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'final_docs'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6184df339032>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtfidfconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.85\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdoc_term_matrix_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidfconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'final_docs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'U'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'final_docs'"
     ]
    }
   ],
   "source": [
    "#create document term matrix with TFIDF\n",
    "\n",
    "#import vectorizing tool (usee TFIDF)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# set max_features to 2000 (specifies the number of most frequently occurring words for which we want to create feature vectors)\n",
    "# set min_df to 5 (word must occur in at least 5 documents)\n",
    "# set max_df to 0.85 (word must not occur in more than 85 percent of the documents) \n",
    "\n",
    "tfidfconverter = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.85, ngram_range=(1, 2), stop_words='english')  \n",
    "doc_term_matrix_1 = tfidfconverter.fit_transform(df['final_docs'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run NMF model \n",
    "\n",
    "#import NMF tool \n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf_model = NMF(n_components=6)\n",
    "nmf_Z = nmf_model.fit_transform(doc_term_matrix_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_term_matrix_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c26aaf9f558f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlda_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'online'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlda_Z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_term_matrix_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'doc_term_matrix_1' is not defined"
     ]
    }
   ],
   "source": [
    "# run LDA model\n",
    "\n",
    "#import LDA tool \n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components = 6, max_iter=10, learning_method='online', learning_decay=.9)\n",
    "lda_Z = lda_model.fit_transform(doc_term_matrix_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Model:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lda_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2261d51dcb7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LDA Model:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidfconverter\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lda_model' is not defined"
     ]
    }
   ],
   "source": [
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]])\n",
    " \n",
    "print(\"LDA Model:\")\n",
    "print_topics(lda_model, tfidfconverter )\n",
    "print(\"=\" * 20)\n",
    " \n",
    "print(\"NMF Model:\")\n",
    "print_topics(nmf_model, tfidfconverter )\n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Run visualization and testing of LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of LDA model \n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda_model, doc_term_matrix_1, tfidfconverter, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test LDA model\n",
    "\n",
    "# log likelihood (higher score is better)\n",
    "print(\"Log Likelihood: \", lda_model.score(doc_term_matrix_1))\n",
    "\n",
    "# perplexity (lower score is better)\n",
    "print(\"Perplexity: \", lda_model.perplexity(doc_term_matrix_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
